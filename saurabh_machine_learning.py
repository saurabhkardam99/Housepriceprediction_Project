# -*- coding: utf-8 -*-
"""saurabh machine learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DCSI_YSIAOk0C2squkxrYr9AeeGvAo6-
"""

# data gathering
import pandas as pd
data = pd.read_csv("https://trello-attachments.s3.amazonaws.com/5d404847c84f418860597cb6/5d85a3f300231d07d1d5adaa/83b939669b19bf1c7126daab925375f1/Housing_Modified.csv")
print("data has %d rows and %d columns" %data.shape)

# shpae of data
print(data.shape)
print("data has %d rows and %d columns"%data.shape)

#Check top 5 values in data
data.head(5)
#check bottom 5 values in data
data.tail(5)

#Access any random row from data
columns=data.columns
data["price"][2]

data[["price","lotsize","airco"]]

# what happens in data ?
import matplotlib.pyplot as plt
plt.scatter(data["lotsize"],data["price"],color="black")
plt.xlabel("lot size(Area of house)")
plt.ylabel("house prices(Area of house)")
plt.title("house prices vs lotsize")

#why did it happens?
data.corr() #calculate correlations

data.head(3)

# Start
import pandas as pd
data = pd.read_csv("https://trello-attachments.s3.amazonaws.com/5d404847c84f418860597cb6/5d85a3f300231d07d1d5adaa/83b939669b19bf1c7126daab925375f1/Housing_Modified.csv")
data.head(3)

# stage 1 Selection
# selection a dependent and independent variables
# check the influence
print(data.columns)
data.corr()

data.columns
temp_data=data[["stories","driveway","recroom","fullbase","gashw","airco","prefarea"]]
temp_data.head(3)
print("stories" ,temp_data["stories"].unique())
print("driveway", temp_data["driveway"].unique())

# stage 2 - preprocessing
# a.missing Data
# b.convert text to Numbers

#check Missing data
data.isna()

# what to do if data is missing ?
# a. drop the null values
data_withoutNA =data.dropna()
# b. fil misising values with mean,median,max,min
# check the mean, median,max and min value of bedrooms
print("max of bedrooms",data["bedrooms"].max())
print("min of bedrooms",data["bedrooms"].min())
print("mean of bedrooms",round(data["bedrooms"].mean()))
print("median of bedrooms",round(data["bedrooms"].median()))

# fill the missing value with max,min,mean,median
data_clean= data.fillna(data["bedrooms"].mean())
data_clean

# convert data into numbers
# a.convert binary category to number(0/1)
# label Binarizer
import sklearn.preprocessing as pp
print(temp_data.columns)

lb =pp.LabelBinarizer()
data.driveway =lb.fit_transform(data.driveway)
data.recroom =lb.fit_transform(data.recroom)
data.fullbase =lb.fit_transform(data.fullbase)
data.gashw =lb.fit_transform(data.gashw)
data.airco =lb.fit_transform(data.airco)
data.prefarea =lb.fit_transform(data.prefarea)
data.head(3)

#convert n-category into numbers using one Hot Encoding
stories_data = pd.get_dummies(data["stories"],prefix="stories")
data=pd.concat([data,stories_data],axis=1)
data.head(3)

data.columns

print("before delete",data.columns)
# delete stories columns
del data["stories"]
print("after delete", data.columns)

# check rhe correlation
data.corr()

# create a correlation maxtrix(color Grid)
import statsmodels.api as sm
sm.graphics.plot_corr(data.corr(),xnames=data.columns,title="HouseMatrix")

# create a correlation maxtrix using seaborn
import seaborn as sb
import matplotlib.pyplot as plt
fig, ax =plt.subplots(figsize=(10,10))
sb.heatmap(data.corr(),annot =True,ax =ax)

data.plot()
print("Price ranges from", data.price.min(), "-", data.price.max())
print("Bedroom ranges from", data.bedrooms.min(), "-", data.bedrooms.max())

# Transformation using Standardization
# Formula = (X - Xmean) / Xstd
 
X = data["price"]
Xmean = X.mean()
print("Mean of price is", Xmean)
Xstd = X.std()
print('Standard deviation is', Xstd)
 
Xnorm = (X - Xmean) / Xstd
print("Normalized price is", Xnorm)

# Standardization of data using Standard Score
standard_data = (data - data.mean())/data.std()

# Plot standardized values
standard_data.plot(figsize=(300, 2))

plt.scatter(data["bedrooms"],data["price"])

ss = pp.StandardScaler()
ss.fit_transform(data)

# Transfrom using min - max scaler
# xnorm = (x-xmin)/ (xmax-xmin)
norm_data  = (data - data.min()) / (data.max()-data.min())
norm_data

# print min max ranges from price and bedrooms
print("price ranges", norm_data["price"].min(),norm_data["price"].max())
print("bedrooms ranges", norm_data["bedrooms"].min(),norm_data["bedrooms"].max())

plt.scatter(data["lotsize"],data["price"])
plt.xlabel("X-axis lotsize(independent variableS)")
plt.ylabel("Y-axis price(dependent variable)")

Y = data["price"]
Y # dependent variable
X = data #independent variables
del X["price"]

regression = sm.OLS(Y, X) # refression equation /model

# train the model 
# calculate the value of attribute using actual values of 
# dependent  and independent variables
# for regrrssion-caclulate the value of coefficient and
# intercept using the actual values of X and Y
model = regression.fit() # train the model

model.summary()

data["predictedprice"]=model.predict()

data

data["price"] = Y
data[["price", "lotsize", "predictedprice"]]

# Comparing Actual prices vs predicted prices
plt.scatter(data["lotsize"], data["price"], color="red")
plt.scatter(data["lotsize"], data["predictedprice"], color="green")
plt.title("Compare Actual price vs Predicted Price")
plt.xlabel("Lotsize")
plt.ylabel("Price")

del X["predictedprice"]
del X["price"]

# Create an AI Console App
user_data = {}
for column in X.columns:
  temp_val = int(input("Enter "+column+": "))
  user_data[column] = temp_val
user_data
input_data = pd.DataFrame(data = user_data, index=[0])
output = model.predict(input_data)
print("Price of the house is USD", output)